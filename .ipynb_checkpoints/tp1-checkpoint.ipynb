{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHDkn9SAt_Dp"
   },
   "source": [
    "# TP 1. Digital House Grupo N7\n",
    "\n",
    "Desafío 1. Análisis exploratorio de un dataset de\n",
    "precios de propiedades\n",
    "\n",
    "**Objetivos:**\n",
    "\n",
    "*   Efectuar una limpieza del dataset provisto. Particularmente, deberá diseñar estrategias para lidiar con los datos perdidos en ciertas variables.\n",
    "*   Realizar un análisis descriptivo de las principales variables.\n",
    "* Crear nuevas columnas a partir de las características dadas que puedan tener valor\n",
    "predictivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56CezKtz5vJH"
   },
   "source": [
    "# Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54U35qeULfr6"
   },
   "outputs": [],
   "source": [
    "#descargo archivos\n",
    "! gdown 1Ui7dofcyVVJlqPSqnJjmufPKnZxw5ZAj #properatti.csv\n",
    "! gdown 1I2V15Sf2mUqXQUQvExfvzipDxnWMqvXM #provincia.shx\n",
    "! gdown 1Im6hNqTDb1zaCX4Lo9czGaMmqgzvDj9S #ar.csv\n",
    "! gdown 1RCIHoEfk07OiSZw5PtOxVdj4aiHxQJpC #provincia.shp\n",
    "! gdown 1EcVqccbr0qdJtuyzOHBRtSkkJZlxf6_Z #provincia.prj\n",
    "! gdown 1r3ElDulmkTvdGwL6zuIWLZ08C8bM4kqw #provincia.dbf\n",
    "! gdown 11VXNu7X_BIfB-2ZtStlxQmHoL_OBdafk #provincia.cpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viOO1B_zsL-r"
   },
   "outputs": [],
   "source": [
    "#instalo dependencias\n",
    "!pip install --upgrade geopandas pyshp shapely descartes\n",
    "!pip install --upgrade unidecode\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from shapely.geometry import Point, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "from unidecode import unidecode\n",
    "import seaborn as sns\n",
    "from importlib.metadata import version\n",
    "version('unidecode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOKK8n3KwDai"
   },
   "outputs": [],
   "source": [
    "#Carga del dataframe\n",
    "properatti_df_original=pd.read_csv('/content/properatti.csv', sep=\",\")\n",
    "properatti_df=properatti_df_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_d9jPJQvFcP"
   },
   "source": [
    "# Análisis Inicial del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwmnOO2ED7TZ"
   },
   "outputs": [],
   "source": [
    "#Configuración para ver todas las columnas del dataframe sin el \"...\"\n",
    "pd.options.display.max_columns = None\n",
    "#Se muestra el head del df\n",
    "properatti_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3mSRgRqpKcM"
   },
   "source": [
    "<h3>Se exploraron las distintas columnas del Dataframe con el atributo  Dataframe.dtypes, el tamaño del dataset(shape), la cantidad de valores por cada columna(count) y </h3>\n",
    "\n",
    "> * Aqui se puede observar que las columnas tienen el tipo de dato esperado. Float64 para valores numericos y objects para columnas de tipo string.\n",
    "> * Hay 121220 fila pero no todas las columnas tienen la misma cantidad de valores.\n",
    "> * No existen filas duplicadas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_n0axK1pLRR"
   },
   "outputs": [],
   "source": [
    "properatti_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ly5gFNFkzS8k"
   },
   "outputs": [],
   "source": [
    "properatti_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31g13Xz5zNUc"
   },
   "outputs": [],
   "source": [
    "properatti_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbfec026"
   },
   "outputs": [],
   "source": [
    "properatti_df[properatti_df.duplicated].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CD4F46B3891A"
   },
   "source": [
    "<h2>Evaluación de los valores nulos en el dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gU1KkfU91yU"
   },
   "outputs": [],
   "source": [
    "porcentaje_de_nulos = properatti_df.isna().sum()/(properatti_df.shape[0])*100\n",
    "porcentaje_de_nulos.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LD3ZWJBa_vF0"
   },
   "source": [
    "Representación gráfica de los valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIn3_yaE95sR"
   },
   "outputs": [],
   "source": [
    "lista=properatti_df.columns.tolist()\n",
    "valores=porcentaje_de_nulos.round(2).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BemLvlcf95um"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(lista, valores, tick_label= lista)\n",
    "\n",
    "ax.set(xlabel = \"Columna\", ylabel = \"Porcentaje de nulos y\", title = \"Porcentaje de nulos en cada columna\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.gcf().set_size_inches(20, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xM68C9hmJ1Pm"
   },
   "source": [
    "Evaluar algunos datos no nulos o con bajo porcentaje de nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XH4fDbkKMjq"
   },
   "outputs": [],
   "source": [
    "properatti_df['operation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8RaiGdyKfIu"
   },
   "outputs": [],
   "source": [
    "properatti_df['properati_url'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EJSLe1lLHQs"
   },
   "outputs": [],
   "source": [
    "properatti_df['image_thumbnail'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-KnE2M2KYLm"
   },
   "outputs": [],
   "source": [
    "properatti_df['property_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bF0UcGkipkLt"
   },
   "source": [
    "\n",
    "<h2> Definición de Estrategias en la Limpieza de Datos</h2>\n",
    "<h4>Se puede observar que las columnas operation, property_type, place_with_parent_names, country_name, state_name, properati_url y title tienen valores no nulos</h4>\n",
    "\n",
    "<h3>Eliminación de datos </h3>\n",
    "\n",
    "><h4><u>image_thumbnail</u></h4>\n",
    "><h5>quitaremos, no es util </h5>\n",
    "\n",
    "><h4><u>properati_url</u></h4>\n",
    "><h5>quitaremos, no es util </h5>\n",
    "\n",
    "><h4><u>Operation</u></h4>\n",
    "><h5>El unico tipo de operacion es 'sell' por lo tanto se eliminara del dataset</h5>\n",
    "\n",
    "<h3>Datos de soporte para otros cálculos</h3>\n",
    "\n",
    "><h4><u>property_type</u></h4>\n",
    "><h5>Veremos los distintos tipos de propiedades, normalizaremos de ser necesario. Esta columna es importante ya que luego para analisis de precio seguramente sea necesario discriminar</h5>\n",
    "\n",
    "><h4><u>description</u></h4>\n",
    "><h5>utilizaremos para buscar info de otras columnas</h5>\n",
    "\n",
    "><h4><u>title</u></h4>\n",
    "><h5>utilizaremos para buscar info de otras columnas</h5>\n",
    "\n",
    "\n",
    "<h3>Completar Datos faltantes </h3>\n",
    "\n",
    "><h4><u>geonames_id, lat-lon, lat y lon</u></h4>\n",
    "><h5>Limpiaremos y utilizaremos esta informacion para visualizar la ubicacion de las propiedades. La informacion de la geografia se complementa con informacion proveniente de un dataframe extra (DATA_GEONAMES) y dos mapas de Argentina, uno con division politica y otro continental.</h5>\n",
    "\n",
    "><h4><u>place_with_parent_names</u></h4>\n",
    "><h5>Normalizaremos esta columna, para obtener informacion acerca de la ubicacion en terminos de provincia, ciudad y barrio. Dicha informacion se guardara en columnas nuevas.</h5>\n",
    "\n",
    "><h4><u>price, currency, price_aprox_local_currency,price_aprox_usd</u></h4>\n",
    "\n",
    "><h5>Normalizaremos en una nueva columna con precio aproximado en usd, utilizando el valor de la columna price_aprox_usd, intentando extraer via regex de la columna descripcion, o convirtiendo los valores en caso de que esten en otra moneda</h5>\n",
    "\n",
    "><h4><u>surface_total_in_m2</u></h4>\n",
    "<h5>Crearemos una nueva columna con los valores de la columna original y extrayendo los faltantes que se puedan encontrar, mediante una regex, en las columnas descripción, title y url.</h5>\n",
    "\n",
    "><h4><u>surface_covered_in_m2</u></h4>\n",
    "><h5>Crearemos una nueva columna con los valores de la columna original y extrayendo los faltantes que se puedan encontrar, mediante una regex, en las columnas descripción, title y url.</h5>\n",
    "\n",
    "><h4><u>price_usd_per_m2</u></h4>\n",
    "<h5>Representa sup_total / precio. calcularemos bonde haga falta con los valores de las columnas correspondientes</h5>\n",
    "\n",
    "><h4><u>price_per_m2</u></h4>\n",
    "><h5>Utilizaremos la column price_usd_per_m2, pero utilizaremos esta si podemos para llenar donde falte price_usd_per_m2</h5>\n",
    "\n",
    "><h4><u>Floor</u></h4>\n",
    "<h5>Veremos si es relevante, trataremos completar con regex </h5>\n",
    "\n",
    "><h4><u>Rooms</u></h4>\n",
    "><h5>Buscaremos con regex, sino veremos correlacion con surface_covered_in_m2 para copmletar</h5>\n",
    "\n",
    "><h4><u>Expenses</u></h4>\n",
    "><h5>Veremos si es relevante, trataremos completar con regex </h5>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kkIXwN8F6D1"
   },
   "source": [
    "# Eliminación de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rV2HgulzMTKP"
   },
   "source": [
    "Eliminar datos no útiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWUhumgRGHjz"
   },
   "outputs": [],
   "source": [
    "#Se eliminan las columnas mencionadas en la sección anterior.\n",
    "\n",
    "properatti_df.drop('operation', axis=1, inplace=True)\n",
    "properatti_df.drop('image_thumbnail', axis=1, inplace=True)\n",
    "properatti_df.drop('properati_url', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "684pjS14MPkJ"
   },
   "source": [
    "Eliminar datos erróneos\n",
    ">Eliminar pubicaciones de pozos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwcFI30YNJGl"
   },
   "outputs": [],
   "source": [
    "#Normalizacion de los datos\n",
    "properatti_df['description'] = properatti_df['description'].str.lower()\n",
    "properatti_df['title'] = properatti_df['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeO1RlJpMcXY"
   },
   "outputs": [],
   "source": [
    "# Se buscan publicaciones que tengan palabras que sugieran publicaciones de tipo lote de multiples propiedades, las cual no nos interesa ya que contaminan el dataset\n",
    "search_strings = [\"unidades\", \"departamentos\", \" uds\", \"deptos\"]\n",
    "\n",
    "# Creo mascara para detectar si la columna title tiene alguna de mis palabras a buscar\n",
    "mask_search_string = properatti_df['title'].str.contains('|'.join(search_strings))\n",
    "\n",
    "# Filtrar el data frame\n",
    "multiple_filter_df = properatti_df[mask_search_string]\n",
    "\n",
    "# Se imprimen las primeras 100 descripciones de entries que cumplan mi mascara para analizarlas\n",
    "row_iterator=0\n",
    "for index, row in multiple_filter_df.iterrows():\n",
    "  if row_iterator>100:\n",
    "    break\n",
    "  print(f\"Index: {index}\\nTitle: {row['title']}\\nDescription: {row['description']}\\nRooms: {row['rooms']}\\nFloor: {row['floor']}\\nPrice: {row['price_aprox_usd']}\\n\\n\")\n",
    "  row_iterator+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfmqA8DXQ4L8"
   },
   "outputs": [],
   "source": [
    "multiple_filter_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfAxY3L9QuOw"
   },
   "source": [
    "><h4>Vemos que la mayoria de estas entries son efectivamente publicaciones de pozo, o de multiples departamentos a la vez, y al ver que son 1313, decidimos retirarlas del Dataframe sobre el cual trabajaremos</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrqSb3VaQmD1"
   },
   "outputs": [],
   "source": [
    "properatti_df = properatti_df[~mask_search_string]\n",
    "print(\"entries restantes:\", properatti_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEwSAKKbRdx6"
   },
   "source": [
    ">Evaluar valores de surface_total_in_m2,surface_covered_in_m2,price_aprox_usd y price erróneos que tienen valores menores a un threshold lógico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ac7Wc45VRn08"
   },
   "outputs": [],
   "source": [
    "print(properatti_df[properatti_df['surface_total_in_m2']<5].shape[0])\n",
    "print(properatti_df[properatti_df['surface_covered_in_m2']<2].shape[0])\n",
    "print(properatti_df[properatti_df['price_aprox_usd']<100].shape[0])\n",
    "print(properatti_df[properatti_df['price']<100].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGI5EAxjSXZQ"
   },
   "source": [
    "Eliminar los valores que esten en dentro del threshold definido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JfPdR6RlSPyy"
   },
   "outputs": [],
   "source": [
    "#convierto con una mascara y una funcion a nan\n",
    "def replace_value_with_nan(x,threshold):\n",
    "    return np.nan if x < threshold else x\n",
    "\n",
    "properatti_df['surface_total_in_m2'] = properatti_df['surface_total_in_m2'].apply(replace_value_with_nan, args=(5,))\n",
    "properatti_df['surface_covered_in_m2'] = properatti_df['surface_covered_in_m2'].apply(replace_value_with_nan, args=(5,))\n",
    "properatti_df['price_aprox_usd'] = properatti_df['price_aprox_usd'].apply(replace_value_with_nan, args=(100,))\n",
    "properatti_df['price'] = properatti_df['price'].apply(replace_value_with_nan, args=(100,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7P9IV3Oh_VYP"
   },
   "source": [
    "# Completar datos faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHL1rwmY953F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Or4d4VJd955d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kftT474m00QO"
   },
   "outputs": [],
   "source": [
    "def calculate_correlation(dataframe, column1, column2, lower_bound1=None, lower_bound2=None, upper_bound1=None, upper_bound2=None):\n",
    "    #Si no paso los limites, los calculo de acuerdo a la formula tradicional\n",
    "    if lower_bound1 is None or upper_bound1 is None:\n",
    "        Q1 = dataframe[column1].quantile(0.25)\n",
    "        Q3 = dataframe[column1].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        if lower_bound1 is None:\n",
    "            lower_bound1 = Q1 - 1.5 * IQR\n",
    "        if upper_bound1 is None:\n",
    "            upper_bound1 = Q3 + 1.5 * IQR\n",
    "\n",
    "    if lower_bound2 is None or upper_bound2 is None:\n",
    "        Q1 = dataframe[column2].quantile(0.25)\n",
    "        Q3 = dataframe[column2].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        if lower_bound2 is None:\n",
    "            lower_bound2 = Q1 - 1.5 * IQR\n",
    "        if upper_bound2 is None:\n",
    "            upper_bound2 = Q3 + 1.5 * IQR\n",
    "\n",
    "    #Filtro outliers\n",
    "    dataframe = dataframe[(dataframe[column1] >= lower_bound1) & (dataframe[column1] <= upper_bound1)]\n",
    "    dataframe = dataframe[(dataframe[column2] >= lower_bound2) & (dataframe[column2] <= upper_bound2)]\n",
    "\n",
    "    #Calculo la correlacion\n",
    "    correlation = dataframe[[column1, column2]].corr().iloc[0,1]\n",
    "\n",
    "    return correlation\n",
    "\n",
    "\n",
    "#correlacion en grupos\n",
    "def calculate_correlation_grouped(df, column1, column2, groupby_column,\n",
    "                           lower_bound1=None, lower_bound2=None,\n",
    "                           upper_bound1=None, upper_bound2=None):\n",
    "\n",
    "    # Agrupo segun la columna pasada como groupby_column\n",
    "    grouped = df.groupby(groupby_column)\n",
    "\n",
    "    correlation_list = []\n",
    "    for name, group in grouped:\n",
    "\n",
    "        #Si no paso los limites, los calculo de acuerdo a la formula tradicional\n",
    "        if lower_bound1 is None or upper_bound1 is None:\n",
    "            Q1 = group[column1].quantile(0.25)\n",
    "            Q3 = group[column1].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            if lower_bound1 is None:\n",
    "                lower_bound1 = Q1 - 1.5 * IQR\n",
    "            if upper_bound1 is None:\n",
    "                upper_bound1 = Q3 + 1.5 * IQR\n",
    "\n",
    "        if lower_bound2 is None or upper_bound2 is None:\n",
    "            Q1 = group[column2].quantile(0.25)\n",
    "            Q3 = group[column2].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            if lower_bound2 is None:\n",
    "                lower_bound2 = Q1 - 1.5 * IQR\n",
    "            if upper_bound2 is None:\n",
    "                upper_bound2 = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Filtro outliers\n",
    "        group = group[(group[column1] > lower_bound1) & (group[column1] < upper_bound1) &\n",
    "                      (group[column2] > lower_bound2) & (group[column2] < upper_bound2)]\n",
    "\n",
    "        # Chequeo si el grupo tiene informacion suficiente para calular la correlacion\n",
    "        if group.shape[0] > 1 and group[column1].nunique() > 1 and group[column2].nunique() > 1:\n",
    "            correlation = group[[column1, column2]].corr().iloc[0,1]\n",
    "            correlation_list.append(correlation)\n",
    "\n",
    "    # chequeo si tengo correlaciones en mi lista, luego calculo la media\n",
    "    if len(correlation_list) > 0:\n",
    "        return np.mean(correlation_list)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZKBXzUFZq3R"
   },
   "outputs": [],
   "source": [
    "#funcion para imputar dato en columna column2 en base a la media para segun column2 cuando existe\n",
    "def get_column2_with_column1(df,column1,column2):\n",
    "\n",
    "    #filtro las entries con column1 valida\n",
    "    valid_rows = df[df[column1].notnull()]\n",
    "\n",
    "    #agrupo por column1 y obtengo la media de column2\n",
    "    mean_column2_by_column1 = valid_rows.groupby(column1)[column2].mean()\n",
    "\n",
    "    #recorro mi df y asigno valor a column2 cuando no tiene valor\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row[column2]):\n",
    "            column1_value = row[column1]\n",
    "            if column1_value in mean_column2_by_column1.index:\n",
    "                mean_column2 = mean_column2_by_column1[column1_value]\n",
    "                # Chequeo si la media != NaN, y asigno valor teniendo en cuanta que la media puede ser un float, y dando mas probabilidad al entero mas cercano, y menor al mas lejano, si mean = 3.1, tendra mas prob\n",
    "                # de imputar 3, que de imputar 4\n",
    "                if not np.isnan(mean_column2):\n",
    "                    probability = mean_column2 - int(mean_column2)\n",
    "                    rounded_column2 = np.random.choice([np.floor(mean_column2), np.ceil(mean_column2)], p=[1 - probability, probability])\n",
    "                    df.at[index, column2] = rounded_column2\n",
    "                else:\n",
    "                    #si la media es NaN, se imputa NaN\n",
    "                    df.at[index, column2] = mean_column2\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6Qw5lLMNHR-"
   },
   "source": [
    "# Ubicaciones (Juan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxE5rWCxiMek"
   },
   "outputs": [],
   "source": [
    "# En el dataframe properatti_df, la informacion relevante para limpiar relacionada a la geografia esta distribuida en 4 columnas:\n",
    "  # geonames_id, lat,\tlon, y\tlat-lon\n",
    "\n",
    "# Se limpiarán los datos relacionados a la geografia, con el objetivo de:\n",
    "  # poder graficar cada propiedad como un punto en un mapa. El mapa puede ser de todo el pais, por provincia o por region.\n",
    "  # poder utilizar datos geograficos para realizar analisis exploratorios (ej., histograma de precios por ciudad, provincia, etc.)\n",
    "\n",
    "# la estrategia para limpiar la geografia va a ser:\n",
    "  # Imputar en la medida de lo posible la informacion de ubicación geografica (latitud y longitud) sobre las columnas 'lat' y 'lon' para todas las propiedades.\n",
    "  # cuando no haya informacion en estos campos, tratar de recolectarla del geodataframe 'DATA_GEONAMES' (https://www.geonames.org/countries/AR/argentina.html)\n",
    "    # Esto se hara de dos maneras:\n",
    "      # la primera, trayendo la informacion de latitud y longitud de DATA_GEONAMES que coincida con el identificador de geonames de la propiedad.\n",
    "      # Cuando esto no sea posible, se entrecruzará la informacion de ciudad o provincia de cada propiedad con la del dataframe DATA_GEONAMES, para traer la informacion de lat y lon correspondiente.\n",
    "      # (esto es una aproximacion, ya que para cada propiedad, los valores de latitud y longitud seran los de la ciudad o provincia correspondiente)\n",
    "    # Para esto, se limpiará la columna 'place_with_parent_names', y la información se distribuirá en cuatro columnas nuevas:\n",
    "      # provincia, partido, ciudad y barrio.\n",
    "\n",
    "properatti_df[['title','geonames_id','lat','lon', 'lat-lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-0RuEcyD_2k"
   },
   "outputs": [],
   "source": [
    "# importo el Mapa de Argentina CON info por provincias\n",
    "\n",
    "data_location = \"/content/provincia.shx\"\n",
    "mapa_argentina = gpd.read_file(data_location)\n",
    "\n",
    "# importo el Mapa (2) de Argentina SIN info por provincias\n",
    "\n",
    "mundo = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n",
    "mapa_argentina_2 = mundo[mundo.name == \"Argentina\"]\n",
    "\n",
    "mapa_argentina.head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sj65LIk9D_2k"
   },
   "outputs": [],
   "source": [
    "# este mapa trae los poligonos de cada provincia.\n",
    "\n",
    "mapa_argentina.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DMkMK2XD_2k"
   },
   "outputs": [],
   "source": [
    "# este mapa no trae info de provincias (mas practico para ploteo general de todas las propiedades)\n",
    "\n",
    "mapa_argentina_2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1_Dl8rfD_2l"
   },
   "outputs": [],
   "source": [
    "# Traemos el geodataframe 'data_geonames'\n",
    "\n",
    "location_geonames = '/content/AR.csv'\n",
    "data_geonames = gpd.read_file(location_geonames, sep='\\t')\n",
    "\n",
    "print(data_geonames.shape)\n",
    "data_geonames.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1bCZ29U0Jax"
   },
   "outputs": [],
   "source": [
    "# Se hacen algunos ajustes al dataframe DATA_GEONAMES:\n",
    "# Primero, convierto a float64 la columna 'GEONAMEID' que es un string. Dicha columna se usara para matchear con 'geonames_id' en properatti_df (float64).\n",
    "\n",
    "data_geonames['GEONAMEID'] = pd.to_numeric(data_geonames['GEONAMEID'], errors='coerce').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHe15MuMD_2l"
   },
   "outputs": [],
   "source": [
    "# Segundo, utilizo la informacion de 'TIMEZONE' para crear una columna llamada 'provincia'. Dicha columna se usara para matchear con 'provincia' en properatti_df (object)).\n",
    "\n",
    "pattern = re.compile(r'/([^/]+)$')\n",
    "\n",
    "def cambiar_nombre(timezone):\n",
    "    match = re.search(pattern, timezone)\n",
    "    return match.group(1).replace('_', ' ') if match else None\n",
    "\n",
    "data_geonames['provincia'] = data_geonames['TIMEZONE'].apply(lambda x: cambiar_nombre(x))\n",
    "data_geonames.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jx55N4sQD_2l"
   },
   "outputs": [],
   "source": [
    "# chequeo de valores nulos en las columnas de la geografia:\n",
    "\n",
    "mask_lat_lon = np.logical_and(properatti_df['lat'].isnull(), properatti_df['lon'].isnull())\n",
    "mask_latlon = properatti_df['lat-lon'].isnull()\n",
    "mask_geonames = properatti_df['geonames_id'].isnull()\n",
    "combined_mask = mask_lat_lon & mask_geonames\n",
    "\n",
    "\n",
    "geografia_filtered_df = properatti_df.loc[mask_lat_lon]\n",
    "print(\"nulls en 'lat' y 'lon': \" f\"{geografia_filtered_df.shape}\")\n",
    "geografia_filtered_df = properatti_df.loc[mask_latlon]\n",
    "print(\"nulls en 'lat-lon': \" f\"{geografia_filtered_df.shape}\")\n",
    "# 'lat', 'lon' y 'lat-lon' tienen la misma info\n",
    "\n",
    "geografia_filtered_df = properatti_df.loc[mask_geonames]\n",
    "print(\"nulls en 'geonames_id': \" f\"{geografia_filtered_df.shape}\")\n",
    "geografia_filtered_df = properatti_df.loc[combined_mask]\n",
    "print(\"ninguna informacion: \" f\"{geografia_filtered_df.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPgsq_iXkl0J"
   },
   "outputs": [],
   "source": [
    "# Estado de los valores nulos al comienzo del proceso.\n",
    "\n",
    "nulls_df = properatti_df[['lat', 'lon', 'lat-lon', 'geonames_id']].isnull()\n",
    "\n",
    "# Plotear los valores nulos en un heatmap abarcando todo el dataframe.\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(nulls_df, cmap='viridis', cbar=False, yticklabels=False)\n",
    "plt.title('Valores Nulos en Columnas de Geografía')\n",
    "plt.show()\n",
    "\n",
    "# Se ve claramente que 'lat', 'lon' y 'lat-lon' son redundantes, es decir tienen la misma informacion. La diferencia es que 'lat-lon' tiene los valores 'lat' y 'lon' en forma de lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9kSZOOJD_2k"
   },
   "outputs": [],
   "source": [
    "# Un pantallazo a la columna 'place_with_parent_names' muestra que es en principio la informacion sobre la provincia (cuando se trata de Buenos Aires, tambien hay informacion sobre el partido), ciudad, y en algunos casos barrio.\n",
    "# Esta informacion me va a servir para cruzar datos con DATA GEONAMES, cuando no haya informacion de 'lat' y 'lon'.\n",
    "\n",
    "properatti_df['place_with_parent_names'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yIkKDHKD_2l"
   },
   "outputs": [],
   "source": [
    "# Chequeo si falta algun dato en 'place_with_parent_names'\n",
    "\n",
    "properatti_df.place_with_parent_names.isnull().sum()\n",
    "\n",
    "# Todos tienen algun tipo de informacion de lugar(pais provincia ciudad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmoaCZ_WD_2l"
   },
   "outputs": [],
   "source": [
    "# Filtro los acentos en DATA_GEONAMES y en properatti_df, para que si hay que matchear, no haya diferencias por acentos.\n",
    "\n",
    "columns_2 = ['place_with_parent_names']\n",
    "columns_1 = ['NAME', 'ASCIINAME', 'provincia']\n",
    "\n",
    "def remove_accents(text):\n",
    "    return unidecode(text) if pd.notna(text) else text\n",
    "\n",
    "properatti_df[columns_2] = properatti_df[columns_2].applymap(remove_accents)\n",
    "data_geonames[columns_1] = data_geonames[columns_1].applymap(remove_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tO2QIYOQD_2m"
   },
   "outputs": [],
   "source": [
    "# Asigno la informacion de 'place_with_parent_name' a tres nuevas columnas: 'provincia', 'ciudad' y 'barrio'\n",
    "\n",
    "pattern = re.compile(r'\\|(?P<pais>[^|]+)\\|(?P<provincia>[^|]+)(?:\\|(?P<ciudad>[^|]+))?(?:\\|(?P<barrio>[^|]+))?')\n",
    "\n",
    "def extract_parts(location_info):\n",
    "    match = pattern.match(location_info)\n",
    "    return {'provincia': match.group('provincia') if match else None, 'ciudad': match.group('ciudad') if match else None, 'barrio': match.group('barrio') if match else None}\n",
    "\n",
    "properatti_df[['provincia', 'ciudad', 'barrio']] = properatti_df['place_with_parent_names'].apply(\n",
    "    lambda x: pd.Series(extract_parts(x))\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uEH8eRJD_2m"
   },
   "outputs": [],
   "source": [
    "# Mando a Capital Federal adentro de la Provincia de Buenos Aires (por razones geograficas -no administrativas-)\n",
    "\n",
    "def clean_capital(row):\n",
    "    if row['provincia'] == 'Capital Federal':\n",
    "        row['provincia'] = 'Buenos Aires'\n",
    "        row['barrio'] = row['ciudad']\n",
    "        row['ciudad'] = 'Ciudad Autonoma de Buenos Aires'\n",
    "    return row\n",
    "\n",
    "properatti_df = properatti_df.apply(clean_capital, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YTPVJhhzDN4"
   },
   "outputs": [],
   "source": [
    "# resultado de limpiar 'place_with_parent_names'\n",
    "\n",
    "properatti_df[['title','provincia','ciudad','barrio']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGBv654vD_2m"
   },
   "outputs": [],
   "source": [
    "# Vemos que cuando la provincia es Buenos Aires, el dato nos queda sucio, con informacion sobre el partido.\n",
    "properatti_df['provincia'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnK28YGrD_2m"
   },
   "outputs": [],
   "source": [
    "# Uso regex para limpiar Buenos Aires, enviando la informacion subsiguiente a una nueva columna 'partido'\n",
    "pattern = re.compile(r'(?P<provincia>Bs\\.As\\.|Buenos Aires)\\s(?P<partido>.+)')\n",
    "\n",
    "def match_location(location_info):\n",
    "    if pd.isna(location_info):  # Chequea valores faltantes\n",
    "        return pd.Series({'provincia': None, 'partido': None})\n",
    "\n",
    "    match = pattern.match(location_info)\n",
    "    if match:\n",
    "        provincia = 'Buenos Aires'\n",
    "        partido = match.group('partido')\n",
    "        return pd.Series({'provincia': provincia, 'partido': partido})\n",
    "    else:\n",
    "        return pd.Series({'provincia': location_info, 'partido': None})\n",
    "\n",
    "\n",
    "properatti_df[['provincia', 'partido']] = properatti_df['provincia'].apply(match_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jG-IRcSyD_2m"
   },
   "outputs": [],
   "source": [
    "# Vemos que ahora, el dato Buenos Aires aparece unificado, y el resto de la informacion aparece en 'partido'\n",
    "properatti_df[['provincia','partido']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6VvrvewD_2m"
   },
   "outputs": [],
   "source": [
    "# Pantallazo al campo 'ciudad' correspondiente a 'provincia' == Buenos Aires\n",
    "properatti_df[properatti_df['provincia'] == 'Buenos Aires']['ciudad'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTTTnH-QD_2n"
   },
   "outputs": [],
   "source": [
    "# Primer paso para imputar valores nulos en 'lat' y 'lon':\n",
    "\n",
    "# Matchea los valores de 'geonames_id' en el dataframe 'data_geonames'\n",
    "# cuando los encuentra, se trae el valor de las columnas 'LATITUDE' y 'LONGITUDE'\n",
    "\n",
    "def normalize_geonameid(row):\n",
    "    if pd.isnull(row['lat']) and pd.isnull(row['lon']):\n",
    "        geoname_id = row['geonames_id']\n",
    "        if geoname_id in data_geonames['GEONAMEID'].values:\n",
    "            matching_row = data_geonames[data_geonames['GEONAMEID'] == geoname_id]\n",
    "            if not matching_row.empty:\n",
    "                row.loc['lat'] = matching_row['LATITUDE'].iloc[0]\n",
    "                row.loc['lon'] = matching_row['LONGITUDE'].iloc[0]\n",
    "\n",
    "    return row\n",
    "\n",
    "properatti_df = properatti_df.apply(normalize_geonameid, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qefNLb-ypTLz"
   },
   "outputs": [],
   "source": [
    "properatti_df[['lat', 'lon']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0OtCWSeD_2n"
   },
   "outputs": [],
   "source": [
    "# Estado de los valores nulos al final del primer paso\n",
    "\n",
    "nulls_df = properatti_df[['lat', 'lon', 'geonames_id']].isnull()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(nulls_df, cmap='viridis', cbar=False, yticklabels=False)\n",
    "plt.title('Valores Nulos en Columnas de Geografía')\n",
    "plt.show()\n",
    "\n",
    "# La reduccion de nulos es considerable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVF5fwiTD_2n"
   },
   "outputs": [],
   "source": [
    "# Segundo paso:\n",
    "# Entrecruzo las columnas 'ciudad' y 'NAME' en DATA_GEONAMES (tiene informacion de ciudad).\n",
    "# Si hay matches, me traigo los valores 'LATITUDE' y 'LONGITUDE' de data_geonames.\n",
    "\n",
    "def find_ciudad_province_in_data_geonames(row):\n",
    "    if pd.isnull(row['lat']) and pd.isnull(row['lon']):\n",
    "        ciudad = row['ciudad']\n",
    "        matching_rows = data_geonames[data_geonames['NAME'].isin([ciudad])]\n",
    "\n",
    "        if not matching_rows.empty:\n",
    "            matching_row = matching_rows.iloc[0]\n",
    "            row['lat'] = pd.to_numeric(matching_row['LATITUDE']).astype(float)\n",
    "            row['lon'] = pd.to_numeric(matching_row['LONGITUDE']).astype(float)\n",
    "\n",
    "    return row\n",
    "\n",
    "properatti_df = properatti_df.apply(find_ciudad_province_in_data_geonames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrPaMWrdD_2n"
   },
   "outputs": [],
   "source": [
    "# La reducción de nulos es significativa.\n",
    "\n",
    "properatti_df[['lat', 'lon']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a89X0uTND_2n"
   },
   "outputs": [],
   "source": [
    "# Tercer paso:\n",
    "# Entrecruzo las columnas 'provincia' (properatti_df) y 'provincia' en DATA_GEONAMES (campo creado a partir de 'TIMEZONE' que tiene info de provincias).\n",
    "# Si hay matches, me traigo los valores 'LATITUDE' y 'LONGITUDE' de data_geonames.\n",
    "\n",
    "def find_ciudad_province_in_data_geonames(row):\n",
    "    if pd.isnull(row['lat']) and pd.isnull(row['lon']):\n",
    "        provincia = row['provincia']\n",
    "        matching_rows = data_geonames[data_geonames['provincia'].isin([provincia])]\n",
    "\n",
    "        if not matching_rows.empty:\n",
    "            matching_row = matching_rows.iloc[0]\n",
    "            row['lat'] = pd.to_numeric(matching_row['LATITUDE']).astype(float)\n",
    "            row['lon'] = pd.to_numeric(matching_row['LONGITUDE']).astype(float)\n",
    "\n",
    "    return row\n",
    "\n",
    "properatti_df = properatti_df.apply(find_ciudad_province_in_data_geonames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhUO01eOD_2n"
   },
   "outputs": [],
   "source": [
    "# No hay mas nulos, por lo tanto todos las propiedades han sido imputadas con algun dato relacionado a su ubicación geográfica.\n",
    "\n",
    "properatti_df[['lat', 'lon']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eVmLjfkD_2o"
   },
   "outputs": [],
   "source": [
    "# creo un geoDataFrame con la info geografica\n",
    "geometry = gpd.points_from_xy(pd.to_numeric(properatti_df.lon), pd.to_numeric(properatti_df.lat))\n",
    "\n",
    "\n",
    "properatti_df = gpd.GeoDataFrame(properatti_df, geometry=geometry)\n",
    "\n",
    "# outlier corregido (aparentemente es un error en el dataframe 'DATA_GEONAMES')\n",
    "\n",
    "properatti_df.at[9761, 'geometry'] = Point(-64.00000, -32.00000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NYBeTf4D_2o"
   },
   "outputs": [],
   "source": [
    "# Objetivo 1: Plot de todas las propiedades:\n",
    "\n",
    "properatti_df_final_geografia = properatti_df[['title','place_with_parent_names','provincia','ciudad','barrio','partido','geometry']]\n",
    "print(\"Shape: \" , properatti_df_final_geografia.shape)\n",
    "print(\"total nulos: \", properatti_df_final_geografia['geometry'].isnull().sum())\n",
    "\n",
    "# Mapa de Argentina\n",
    "ax = mapa_argentina_2.plot(color='white', edgecolor='black')\n",
    "\n",
    "# Sobre las ciudades superpone el mapa del país\n",
    "properatti_df.plot(ax=ax, color='red', markersize=0.05)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGETqF6xD_2o"
   },
   "outputs": [],
   "source": [
    "# Objetivo 1.1: Plot por provincia. En este caso, (ej.) San Luis.\n",
    "\n",
    "mapa_san_luis = mapa_argentina[mapa_argentina.fna == \"Provincia de San Luis\"]\n",
    "\n",
    "  # Para plotear, filtro los puntos que estan dentro de 'mapa_san_luis'\n",
    "\n",
    "contained_points = properatti_df[properatti_df.geometry.within(mapa_san_luis.unary_union)]\n",
    "\n",
    "# plot del mapa_san_luis\n",
    "ax = mapa_san_luis.plot(figsize=(10, 10), color='lightgray', edgecolor='black')\n",
    "contained_points.plot(ax=ax, color='red', marker='o', markersize=5, alpha=0.5)\n",
    "plt.title('Propiedades en San Luis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGGVJoaP4G4J"
   },
   "outputs": [],
   "source": [
    "# Objetivo 2: Histograma de precios en tres barrios de Mar del Plata:\n",
    "\n",
    "# Barrios específicos en Mar del Plata\n",
    "barrios = ['Centro', 'Plaza Mitre', 'Chauvin']\n",
    "\n",
    "# Filtra el DataFrame original para incluir solo las propiedades de esos barrios en Mar del Plata\n",
    "precios_barrios = properatti_df[(properatti_df['ciudad'] == 'Mar del Plata') & (properatti_df['barrio'].isin(barrios))]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.violinplot(data=precios_barrios, x='barrio', y='price', palette='Set3')\n",
    "\n",
    "plt.title('Histograma Superpuesto de Precios en Barrios de Mar del Plata')\n",
    "plt.xlabel('Precio')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.legend(title='Barrio')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVyzLfEZK_nn"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hX26HGwgftTF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HJWWlTVK__0"
   },
   "source": [
    "#Limpieza de la columna \"surface_total_in_m2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XzV4b2f-LHmO"
   },
   "outputs": [],
   "source": [
    "#cuento las cantidad de valores nulos de la columa superficie total\n",
    "df_surface_total = properatti_df['surface_total_in_m2']\n",
    "print('Nulos en Surface Total:', df_surface_total.isnull().sum())\n",
    "\n",
    "#calculo el porcentaje de nulos en Superficie Total\n",
    "print('Procentaje de nulos en Surface Total:', (df_surface_total.isnull().sum()/ properatti_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPLvZGrTLgqE"
   },
   "outputs": [],
   "source": [
    "# separlo las columnas en las que se buscará la información faltante\n",
    "df_description = properatti_df['description']\n",
    "df_title = properatti_df['title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QoqpNa-Lgio"
   },
   "outputs": [],
   "source": [
    "#me fijo cuantos valores nulos hay en las columnas donde se buscará la información\n",
    "print('Nulos en Description:', df_description.isnull().sum())\n",
    "\n",
    "print('Nulos en Title:', df_title.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zwSZNb7LgeO"
   },
   "outputs": [],
   "source": [
    "#busco una regex para ver en que descripción/title/url hablan de superficie\n",
    "\n",
    "surface_pattern = r\"(\\d+(?:[,.]\\d{1,2})?)\\s*(m²|metros|m2|mts2|mt2|metros cuadrados|mtrs)\"\n",
    "\n",
    "regex = re.compile(surface_pattern, re.IGNORECASE)\n",
    "# pruebo si la regex funciona en una descrpción en la que se que hablan de m2\n",
    "\n",
    "print(regex.search(df_description.iloc[15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JlyVIqyLgH6"
   },
   "outputs": [],
   "source": [
    "#busco una descripción que hable de metros cuadrados para probar la regex\n",
    "\n",
    "print(df_description.iloc[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmgCRb07LwJG"
   },
   "outputs": [],
   "source": [
    "# pruebo si la regex funciona en una descrpción en la que se que hablan de m2\n",
    "\n",
    "print(regex.search(df_description.iloc[15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jxgodu1LwHF"
   },
   "outputs": [],
   "source": [
    "#pruebo la regex en un texto generado por mi\n",
    "texto = 'dakfj jdhf urhfsdfh jhdfa fp 245 metros x 80 metros cuadrados djdfhsjkdhf'\n",
    "\n",
    "prueba = re.findall(surface_pattern, texto)\n",
    "\n",
    "print(prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "483dZSitLwEt"
   },
   "outputs": [],
   "source": [
    "#filtro el data frame\n",
    "df_filtrado = properatti_df[(properatti_df['surface_total_in_m2'].isnull())]\n",
    "\n",
    "#veo el tamaño del dataframe filtrado\n",
    "print(df_filtrado.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGcj21MELwCK"
   },
   "outputs": [],
   "source": [
    "#filtro el dataframe donde la regex encuantra valores y el cuantas encontró\n",
    "df_filtrado_2 = df_filtrado[\n",
    "    df_filtrado[\"description\"].str.contains(surface_pattern) |\n",
    "    df_filtrado[\"title\"].str.contains(surface_pattern)\n",
    "]\n",
    "\n",
    "print(df_filtrado_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYrL7_qRMSic"
   },
   "outputs": [],
   "source": [
    "#defino una función para aplicar la regex\n",
    "def extract_largest_value(text, regex_pattern):\n",
    "    matches = re.findall(regex_pattern, text)\n",
    "\n",
    "    if matches:\n",
    "        valores_numericos = [float(match[0].replace(',', '.')) for match in matches]\n",
    "        max_valor = max(valores_numericos)\n",
    "        return str(max_valor)\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjGLf0wHMSgN"
   },
   "outputs": [],
   "source": [
    "#agrego 3 columnas con los valores encontrados por la regex\n",
    "print(df_filtrado_2.shape)\n",
    "df_filtrado_2.loc[:, 'description_short'] = df_filtrado_2['description'].apply(lambda x: extract_largest_value(x, surface_pattern))\n",
    "df_filtrado_2.loc[:, 'title_short'] = df_filtrado_2['title'].apply(lambda x: extract_largest_value(x, surface_pattern))\n",
    "print(df_filtrado_2.shape)\n",
    "print(df_filtrado_2[\"description_short\"])\n",
    "print(df_filtrado_2[\"title_short\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEPI2nFjMSdl"
   },
   "outputs": [],
   "source": [
    "#defino una función para aplicar otra regex y solo quedamre con los valores numerícos eliminando los m2, metros o metros cuadrados\n",
    "def extract_number_from_string(string):\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', str(string))\n",
    "    return float(match.group(1)) if match else pd.NA\n",
    "\n",
    "#defino una función para juntar los valores obtenidos con los originales del data frame\n",
    "def fill_null(row):\n",
    "    if pd.isnull(row['surface_total_in_m2']):\n",
    "        corresponding_value = df_filtrado_2.loc[df_filtrado_2['index_column'] == row['index_column'], 'description_short'].values\n",
    "        if len(corresponding_value) > 0:\n",
    "            return extract_number_from_string(corresponding_value[0])\n",
    "    return row['surface_total_in_m2']\n",
    "\n",
    "#Agrego una columna de índice común a ambos DataFrames para machear filas\n",
    "properatti_df['index_column'] = properatti_df.index\n",
    "df_filtrado_2['index_column'] = df_filtrado_2.index\n",
    "\n",
    "#aplico la función y creo la columna 'surface_covered_clean' con los valores\n",
    "properatti_df['surface_total_clean'] = properatti_df.apply(fill_null, axis=1)\n",
    "\n",
    "#elimino la columna de índice temporal\n",
    "properatti_df.drop('index_column', axis=1, inplace=True)\n",
    "df_filtrado_2.drop('index_column', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9O3qoEAMSa4"
   },
   "outputs": [],
   "source": [
    "#veo cuantos nulos quedaron en la nueva columna y comparo con la orginal\n",
    "properatti_df['surface_total_clean'] = pd.to_numeric(properatti_df['surface_total_clean'],errors='coerce')\n",
    "print('Nulos en la Surface_clean:', properatti_df['surface_total_clean'].isnull().sum())\n",
    "print('Nulos en Surface original', properatti_df['surface_total_in_m2'].isnull().sum())\n",
    "print('Cantidad de valores agregados:',properatti_df['surface_total_in_m2'].isnull().sum() - properatti_df['surface_total_clean'].isnull().sum())\n",
    "properatti_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGK6B6JWrmp1"
   },
   "outputs": [],
   "source": [
    "#redondeo los valores con decimal de \"surface_total_in_m2\", \"surface_clean_in_m2\", \"surface_covered_in_m2\"\n",
    "properatti_df['surface_total_in_m2'] = properatti_df['surface_total_in_m2'].round()\n",
    "properatti_df['surface_total_clean'] = properatti_df['surface_total_clean'].round()\n",
    "properatti_df['surface_covered_in_m2'] = properatti_df['surface_covered_in_m2'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4awx4IMLv_v"
   },
   "outputs": [],
   "source": [
    "#calculo la media de las superficies\n",
    "properatti_df['surface_total_clean'].mean()\n",
    "print(properatti_df[properatti_df['surface_total_clean']<10]['surface_total_clean'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kp503KvXFgg"
   },
   "outputs": [],
   "source": [
    "#convierto a nan los entries de menos de 10m2\n",
    "properatti_df['surface_total_clean'] = properatti_df['surface_total_clean'].apply(replace_value_with_nan, args=(5,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SdCvlDl_kCd"
   },
   "source": [
    "# Rooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccd90684"
   },
   "source": [
    "<h1>VEMOS 73830 DATOS NO VALIDOS</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "270cba4b"
   },
   "outputs": [],
   "source": [
    "filtered_df = properatti_df[(properatti_df['rooms'].isnull())]\n",
    "\n",
    "# Filtramos los datos no validos\n",
    "print(filtered_df.shape[0])\n",
    "print(filtered_df['rooms'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c312971"
   },
   "outputs": [],
   "source": [
    "pattern_space_between = re.compile(r'(\\d+)\\s*amb')\n",
    "pattern_space_after_dot = re.compile(r'\\.(?!\\s)')\n",
    "\n",
    "def remove_whitespace_between_number_and_amb(text):\n",
    "    # defino regex para matchear\"<numero> amb\" coi nespacio opcional\n",
    "    result = re.sub(pattern_space_after_dot, '. ', text)\n",
    "    # Uso re.sub para reemplazar matcheo con  \"<numero>amb\"\n",
    "    result = re.sub(pattern_space_between, r'\\1amb', result)\n",
    "    return result\n",
    "\n",
    "#definimos funcion para reemplazar palabras en determinada columna\n",
    "def replace_words_in_column(df, column_name, word_replacements):\n",
    "    for word, replacement in word_replacements:\n",
    "        df[column_name] = df[column_name].str.replace(rf'{word}', str(replacement))\n",
    "    return df\n",
    "\n",
    "# definimos funcion para extraer el mayor numero de mis matches\n",
    "def extract_biggest_number(text, regex_pattern):\n",
    "  matches = re.findall(regex_pattern, text)\n",
    "  if matches:\n",
    "    numbers = [int(n) for n in matches if int(n)<15]\n",
    "    if numbers:\n",
    "      return max(numbers)\n",
    "    else:\n",
    "      return \"\"\n",
    "  else:\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41f2d6d1"
   },
   "outputs": [],
   "source": [
    "#definimos las palabras a reemplazar\n",
    "word_replacements = [\n",
    "    ('un ', 1),\n",
    "    ('mono', 1),\n",
    "    ('dos ', 2),\n",
    "    ('tres ', 3),\n",
    "    ('cuatro ', 4),\n",
    "    ('cinco ', 5),\n",
    "    ('seis ', 6),\n",
    "    ('siete ', 7),\n",
    "    ('ocho ', 8),\n",
    "    ('nueve ', 9)\n",
    "]\n",
    "#aplicamos nuestro filtro a title\n",
    "filtered_df = replace_words_in_column(filtered_df,\"title\",word_replacements)\n",
    "#aplicamos la remocion de whitespaces entre numeros y amb\n",
    "filtered_df['title'] = filtered_df['title'].apply(lambda x: remove_whitespace_between_number_and_amb(x))\n",
    "\n",
    "#aplicamos nuestro filtro a description\n",
    "filtered_df = replace_words_in_column(filtered_df,\"description\",word_replacements)\n",
    "#aplicamos la remocion de whitespaces entre numeros y amb\n",
    "filtered_df['description'] = filtered_df['description'].apply(str).apply(lambda x: remove_whitespace_between_number_and_amb(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89ff52a8"
   },
   "source": [
    "<h1>Normalizamos title para que contenga {numero}amb</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28f7696b"
   },
   "outputs": [],
   "source": [
    "#pattern para encontrar{numero}amb\n",
    "pattern = re.compile(r'(\\d+)amb')\n",
    "\n",
    "filtered_df['title_room_number'] = filtered_df['title'].apply(lambda x: extract_biggest_number(x, pattern))\n",
    "filtered_df['description_room_number'] = filtered_df['description'].apply(lambda x: extract_biggest_number(x, pattern))\n",
    "print(filtered_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1c1c41c2"
   },
   "outputs": [],
   "source": [
    "filtered_df['title_room_number'] = pd.to_numeric(filtered_df['title_room_number'], errors='coerce')\n",
    "filtered_df['description_room_number'] = pd.to_numeric(filtered_df['description_room_number'], errors='coerce')\n",
    "filtered_df['rooms'] = filtered_df['rooms'].combine_first(filtered_df['title_room_number'])\n",
    "filtered_df['rooms'] = filtered_df['rooms'].combine_first(filtered_df['description_room_number'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3c980f5"
   },
   "outputs": [],
   "source": [
    "value_counts = filtered_df['rooms'].value_counts()\n",
    "print(value_counts)\n",
    "print(properatti_df['rooms'].isnull().sum())\n",
    "properatti_df['rooms_clean'] = properatti_df['rooms'].combine_first(filtered_df['rooms'])\n",
    "print(properatti_df['rooms_clean'].isnull().sum())\n",
    "get_column2_with_column1(properatti_df,'surface_covered_in_m2','rooms_clean')\n",
    "print(properatti_df['rooms_clean'].isnull().sum())\n",
    "value_counts = properatti_df['rooms_clean'].value_counts()\n",
    "print(f\"Departamentos sin valores en rooms:\", len(properatti_df[(properatti_df['rooms_clean'].isnull()) & (properatti_df['property_type'] == 'apartment')]))\n",
    "#properatti_df[properatti_df['rooms_clean'].isnull()]['property_type'].value_counts()\n",
    "get_column2_with_column1(properatti_df,'surface_total_clean','rooms_clean')\n",
    "print(f\"Departamentos sin valores en rooms:\", len(properatti_df[(properatti_df['rooms_clean'].isnull()) & (properatti_df['property_type'] == 'apartment')]))\n",
    "properatti_df[properatti_df['rooms_clean'].isnull()]['property_type'].value_counts()\n",
    "get_column2_with_column1(properatti_df,'rooms_clean','surface_total_clean')\n",
    "get_column2_with_column1(properatti_df,'rooms_clean','surface_covered_in_m2')\n",
    "#for index, row in properatti_df[properatti_df['rooms_clean'].isnull()].iterrows():\n",
    " # print(f\"Index: {index}\\nDescription: {row['description']}\\nTitle: {row['title']}\\nProperty type: {row['property_type']}\\nRooms: {row['rooms']}\\nClean: {row['surface_total_clean']}\\nCovered: {row['surface_covered_in_m2']}\\nTotal: {row['surface_total_in_m2']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2omSJkSep00"
   },
   "source": [
    "<h3>Podemos ver que recuperamos aprox 28000 valores de los 73156 que nos faltaban."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D2VfH1iaLcZ"
   },
   "source": [
    "# Limpieza de la columna \"price\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89be090e"
   },
   "outputs": [],
   "source": [
    "#Selecciono las columnas relacionadas con precio como currency, precio, precio aprox usd etc#\n",
    "\n",
    "precios=properatti_df.loc[:,[\"price\",\"currency\",\"price_aprox_local_currency\",\"price_aprox_usd\",\"price_usd_per_m2\",\"price_per_m2\"]]\n",
    "precios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf938a79"
   },
   "outputs": [],
   "source": [
    "precios['Index'] = range(1, len(precios)+1)\n",
    "precios[\"Index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59f16d44"
   },
   "outputs": [],
   "source": [
    "#Calculo cantidad de nulos en precio. siguiente 3 comandos#\n",
    "\n",
    "nulos=pd.isnull(properatti_df[\"price\"]).sum()\n",
    "nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "377d5188"
   },
   "outputs": [],
   "source": [
    "porcentaje_nulos=nulos/(properatti_df[\"price\"].shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "314e26d3"
   },
   "outputs": [],
   "source": [
    "porcentaje_nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "779c6e0e"
   },
   "outputs": [],
   "source": [
    "nulos.dtype\n",
    "\n",
    "is_notnull_result = precios.notnull()\n",
    "canti_not_null=is_notnull_result.sum()\n",
    "print(canti_not_null)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9b597db"
   },
   "outputs": [],
   "source": [
    "porcentaje_no_nulos=canti_not_null/(properatti_df.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68c7d53d"
   },
   "outputs": [],
   "source": [
    "porcentaje_no_nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b5864b8"
   },
   "outputs": [],
   "source": [
    "precios.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8953cbd1"
   },
   "outputs": [],
   "source": [
    "#corroborar que hay nulos en precios#\n",
    "precios[\"price\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb16fd05"
   },
   "outputs": [],
   "source": [
    "#contar cantidad de nulos#\n",
    "contar_valores=properatti_df[\"price\"].value_counts(dropna=False)\n",
    "contar_valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92e96fca"
   },
   "outputs": [],
   "source": [
    "#Tipos de monedas que componen el campo currency#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eb888deb"
   },
   "outputs": [],
   "source": [
    "df_currency_type = properatti_df.groupby(\"currency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2014fb1"
   },
   "outputs": [],
   "source": [
    "df_currency_type_values = list(df_currency_type.groups.keys())\n",
    "df_currency_type_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11c0cdfe"
   },
   "outputs": [],
   "source": [
    "#Cantidad de entries con cada campo#\n",
    "\n",
    "precio_por_moneda=properatti_df.groupby(\"currency\")[\"price\"].count()\n",
    "precio_por_moneda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8d19f4b"
   },
   "outputs": [],
   "source": [
    "mascara_uyu=precios[\"currency\"]==\"UYU\"\n",
    "fila_uy=precios[mascara_uyu]\n",
    "fila_uy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38282639"
   },
   "outputs": [],
   "source": [
    "mascara_pen=precios[\"currency\"]==\"PEN\"\n",
    "fila_pen=precios[mascara_pen]\n",
    "fila_pen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ea030892"
   },
   "outputs": [],
   "source": [
    "valores=properatti_df[mascara_uyu]\n",
    "pd.set_option(\"display.max_colwidth\",1000)\n",
    "print(valores[\"description\"])\n",
    "print(valores[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7444d438"
   },
   "outputs": [],
   "source": [
    "valor_pesos=precios[mascara_uyu]\n",
    "valor_pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11782bb0"
   },
   "outputs": [],
   "source": [
    "#Filtramos los datos de descripcion por moneda#\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71d8e6bb"
   },
   "outputs": [],
   "source": [
    "data_filtrada = properatti_df[properatti_df['price'].isnull()]\n",
    "regex_pattern_2 = re.compile(r'\\b(?:usd|us\\$|u\\$s|\\$|ars)\\b', re.IGNORECASE)\n",
    "filtered_df_2 = data_filtrada[\n",
    "    data_filtrada[\"description\"].str.contains(regex_pattern_2) |\n",
    "    data_filtrada[\"title\"].str.contains(regex_pattern_2)\n",
    "]\n",
    "\n",
    "print(filtered_df_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXxFzrymF9GU"
   },
   "source": [
    "hay 3695 lineas filtradas que corresponden a valores de precio nulos y que se pueden rellenar\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "701db83a"
   },
   "outputs": [],
   "source": [
    "# Define a function to extract substrings around the regex pattern\n",
    "def extract_substring(text, regex_pattern):\n",
    "    match = re.search(regex_pattern, text)\n",
    "    if match:\n",
    "        start_index = max(0, match.start() - 5)\n",
    "        end_index = match.end() + 10\n",
    "        return text[start_index:end_index]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "# Use .loc to set values in the DataFrame\n",
    "print(filtered_df_2.shape)\n",
    "filtered_df_2.loc[:, 'description_short'] = filtered_df_2['description'].apply(lambda x: extract_substring(x, regex_pattern_2))\n",
    "filtered_df_2.loc[:, 'title_short'] = filtered_df_2['title'].apply(lambda x: extract_substring(x, regex_pattern_2))\n",
    "print(filtered_df_2.shape)\n",
    "print(filtered_df_2[\"description\"])\n",
    "print(*filtered_df_2[\"title_short\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d023c5a9"
   },
   "outputs": [],
   "source": [
    "filtered_df_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1805ad3"
   },
   "outputs": [],
   "source": [
    "#Luego de filtrados los datos por moneda, los filtramos por moneda de vuelta, por . y por cantidad de 5 digitos o màs (para descartar que se pusieran datos que correspondan a expensas u otras cosas )\n",
    "\n",
    "pattern_regex=\"U\\$S\\s|usd|us\\$*\"\n",
    "dolar_regex=re.compile(pattern_regex)\n",
    "decimal_regex=\"\\.\"\n",
    "compile_decimal=re.compile(decimal_regex)\n",
    "\n",
    "sin_dolar = filtered_df_2.description_short.apply(lambda x: x if x is np.NaN else dolar_regex.sub(\"\", x))\n",
    "sin_dolar.head(10)\n",
    "sin_punto=sin_dolar.apply(lambda x: x if x is np.NaN else compile_decimal.sub(\"\", x))\n",
    "sin_punto.head(10)\n",
    "\n",
    "pattern_numero=r'(\\d{5,})'\n",
    "regex_numero=re.compile(pattern_numero)\n",
    "cinco_digitos=sin_punto.str.extract(pattern_numero).astype(float)\n",
    "print(cinco_digitos)\n",
    "cinco_digitos.isnull().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "679e64ee"
   },
   "outputs": [],
   "source": [
    "#En base a los anteriores filtrados y a descartar la cantidad de entries que no cumplen con el campo quedan 580 entries en descripcion que permiten llenar el campo precio#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a51de2e9"
   },
   "outputs": [],
   "source": [
    "nulos_descripcion=cinco_digitos.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4a22dfdf"
   },
   "outputs": [],
   "source": [
    "print(nulos_descripcion.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "575148f5"
   },
   "outputs": [],
   "source": [
    "precios[\"price_clean\"]=precios[\"price\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fc9e38b"
   },
   "outputs": [],
   "source": [
    "precios[\"price_clean\"].head(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "582bf8cb"
   },
   "outputs": [],
   "source": [
    "print(nulos_descripcion[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c465c90"
   },
   "outputs": [],
   "source": [
    "precios['Clean_precios'] = precios[\"price_clean\"].fillna(nulos_descripcion.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95c95f8c"
   },
   "outputs": [],
   "source": [
    "precios[\"Clean_precios\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "af43100b"
   },
   "outputs": [],
   "source": [
    "nulos_descripcion.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48cf2b6f"
   },
   "outputs": [],
   "source": [
    "pattern_regex=\"U\\$S\\s|usd|us\\$*\"\n",
    "dolar_regex=re.compile(pattern_regex)\n",
    "decimal_regex=\"\\.\"\n",
    "compile_decimal=re.compile(decimal_regex)\n",
    "\n",
    "sin_dolar = filtered_df_2.title_short.apply(lambda x: x if x is np.NaN else dolar_regex.sub(\"\", x))\n",
    "sin_dolar.head(10)\n",
    "sin_punto=sin_dolar.apply(lambda x: x if x is np.NaN else compile_decimal.sub(\"\", x))\n",
    "sin_punto.head(10)\n",
    "\n",
    "pattern_numero=r'(\\d{5,})'\n",
    "regex_numero=re.compile(pattern_numero)\n",
    "cinco_digitos_ti=sin_punto.str.extract(pattern_numero).astype(float)\n",
    "print(cinco_digitos_ti)\n",
    "cinco_digitos_ti.isnull().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b5a4284"
   },
   "outputs": [],
   "source": [
    "#Se hizo el mismo protocolo para el titulo (el ti de los comandos hace referencia a eso) se encontraron 2412 campos que permitirian llenar#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30cd2dda"
   },
   "outputs": [],
   "source": [
    "nulos_descripcion_ti=cinco_digitos_ti.dropna()\n",
    "nulos_descripcion_ti.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f5de2b1"
   },
   "outputs": [],
   "source": [
    "precios['Clean_precio_titulo'] = precios[\"Clean_precios\"].fillna(nulos_descripcion_ti.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "696093b6"
   },
   "outputs": [],
   "source": [
    "precios[\"Clean_precio_titulo\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86b13406"
   },
   "outputs": [],
   "source": [
    "#mismo protocolo para el caso de la URL. se encontraron 0 campos que permite llenar#\n",
    "\n",
    "\n",
    "sin_punto=sin_dolar.apply(lambda x: x if x is np.NaN else compile_decimal.sub(\"\", x))\n",
    "sin_punto.head(10)\n",
    "\n",
    "pattern_numero=r'(\\d{5,})'\n",
    "regex_numero=re.compile(pattern_numero)\n",
    "cinco_digitos_url=sin_punto.str.extract(pattern_numero).astype(float)\n",
    "print(cinco_digitos_url)\n",
    "cinco_digitos_url.isnull().count()\n",
    "nulos_descripcion_url=cinco_digitos_url.dropna()\n",
    "nulos_descripcion_url.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEY6GzshH3ie"
   },
   "source": [
    "Una vez hecho eso, pensaba en funcion de los campos\n",
    "price, price_aprox_local_currency, price_aprox_USD y currency estimar  un promedio de la cotizacion del dolar\n",
    "para, de esa forma, poder normalizar todas las cotizaciones a USD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geb74WtLHZzj"
   },
   "source": [
    "para hacer lo anterior hago una mascara con currency ars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18f06ccc"
   },
   "outputs": [],
   "source": [
    "mascara_ars=precios[\"currency\"]==\"ARS\"\n",
    "entries_en_ARS=precios[mascara_ars]\n",
    "entries_en_ARS.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a72ee05f"
   },
   "outputs": [],
   "source": [
    "precios[\"conversion_en_USD\"]=entries_en_ARS[\"price_aprox_local_currency\"]/entries_en_ARS[\"price_aprox_usd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab7f55fd"
   },
   "outputs": [],
   "source": [
    "precios[\"conversion_en_USD\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ef34b1b"
   },
   "outputs": [],
   "source": [
    "descarto_nulos_conversion_USD=precios[\"conversion_en_USD\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f77054ad"
   },
   "outputs": [],
   "source": [
    "descarto_nulos_conversion_USD.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9b338f85"
   },
   "outputs": [],
   "source": [
    "descarto_nulos_conversion_USD.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VAwrPxGp6ZF0"
   },
   "outputs": [],
   "source": [
    "#De acuerdo con este promedio 1 dolar corresponde a 17,6445 pesos Se podrìa comprobar lo mismo con las entradas correspondientes al campo currency USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12f9d33d"
   },
   "outputs": [],
   "source": [
    "mascara_usd=precios[\"currency\"]==\"USD\"\n",
    "entries_en_usd=precios[mascara_usd]\n",
    "entries_en_usd.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b22e99cc"
   },
   "outputs": [],
   "source": [
    "precios[\"conversion_en_USD_currency_USD\"]=entries_en_usd[\"price_aprox_local_currency\"]/entries_en_usd[\"price_aprox_usd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "084e01c8"
   },
   "outputs": [],
   "source": [
    "descarto_nulos_conversion_USD_currency_USD=precios[\"conversion_en_USD_currency_USD\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8aef958"
   },
   "outputs": [],
   "source": [
    "descarto_nulos_conversion_USD_currency_USD.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5a479156"
   },
   "outputs": [],
   "source": [
    "descarto_nulos_conversion_USD_currency_USD.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPBa96Nj6eO8"
   },
   "outputs": [],
   "source": [
    "#De acuerdo con este promedio 1 dolar corresponde a 17,6445 pesos\n",
    "\n",
    "#Valor con las entradas correspondientes al campo currency USD\n",
    "#Se pueden normalizar los entries por ese valor de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QZm8SXc6lHH"
   },
   "outputs": [],
   "source": [
    "#conversion de los campos del dataframe PRecios que tiene como moneda ARS a USD\n",
    "#a la vez se corrigen los valores de ARS#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43c8e788"
   },
   "outputs": [],
   "source": [
    "precios.loc[mascara_ars,\"Clean_precio_titulo\"]/=(descarto_nulos_conversion_USD.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aa1ac160"
   },
   "outputs": [],
   "source": [
    "precios[mascara_ars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "515f22d3"
   },
   "outputs": [],
   "source": [
    "precios[\"currency_new\"]=precios[\"currency\"].replace(\"ARS\",\"USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55931b62"
   },
   "outputs": [],
   "source": [
    "precios[\"currency_new\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20646f70"
   },
   "outputs": [],
   "source": [
    "precios = precios.rename(columns={'Clean_precio_titulo': 'precios_nuevos'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45ca7276"
   },
   "outputs": [],
   "source": [
    "precios.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWze7OGQG_Z2"
   },
   "source": [
    "Cambio todos los valores del campo currency_new a USD#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ecdf5ae"
   },
   "outputs": [],
   "source": [
    "fill=\"USD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1a1a622b"
   },
   "outputs": [],
   "source": [
    "precios['currency_new'] = precios.apply(lambda row: fill if pd.isna(row['currency_new']) and pd.notna(row['precios_nuevos']) else row['currency_new'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cjEOC5rHIX_"
   },
   "source": [
    "Por ùltimo tomo en cuenta los valores de los campos correspondientes a las sntradas de currency en UYU y en PEN\n",
    "y las normalizo a USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1968ebd8"
   },
   "outputs": [],
   "source": [
    "precios[mascara_uyu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35f54be9"
   },
   "outputs": [],
   "source": [
    "fila_uy[\"price_aprox_local_currency\"]/fila_uy[\"price_aprox_usd\"]\n",
    "masc_uyu=properatti_df[\"currency\"]==\"UYU\"\n",
    "properatti_df[masc_uyu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16e5fabf"
   },
   "outputs": [],
   "source": [
    "fila_uy[\"price_aprox_local_currency\"]/fila_uy[\"price\"]\n",
    "fila_uy[\"price\"]/fila_uy[\"price_aprox_usd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfe627eb"
   },
   "outputs": [],
   "source": [
    "new_price_UYUtoUSD=fila_uy[\"price_aprox_usd\"]\n",
    "new_price_UYUtoUSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fad89ae6"
   },
   "outputs": [],
   "source": [
    "precios[\"precios_nuevos\"][107391]=new_price_UYUtoUSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6a535d2c"
   },
   "outputs": [],
   "source": [
    "precios[\"precios_nuevos\"][107391]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6717926"
   },
   "outputs": [],
   "source": [
    "fila_pen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aa74a0f"
   },
   "outputs": [],
   "source": [
    "fila_pen[\"price\"]/fila_pen[\"price_aprox_usd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59f19dd0"
   },
   "outputs": [],
   "source": [
    "fila_pen[\"price_aprox_local_currency\"]/fila_pen[\"price_aprox_usd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a3885a5"
   },
   "outputs": [],
   "source": [
    "new_price_PENtoUSD=fila_pen[\"price_aprox_usd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "191d5c80"
   },
   "outputs": [],
   "source": [
    "precios.loc[mascara_pen,\"precios_nuevos\"]=new_price_PENtoUSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b21ea9b9"
   },
   "outputs": [],
   "source": [
    "precios['currency_new'].replace(\"PEN\",\"USD\")\n",
    "precios[\"currency_new\"].replace(\"UYU\",\"USD\")\n",
    "precios[\"currency_new\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzFsXx4XHLb0"
   },
   "source": [
    "Luego de seleccionadas las columnas correpsondeintes a price aprox en USD para los casos de pesos UYU y de PEN se\n",
    "unificaron los currency en la columna currency para establecer su normalizacion a USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a9ec66b"
   },
   "outputs": [],
   "source": [
    "precios.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ef7c128"
   },
   "outputs": [],
   "source": [
    "precios=precios.drop(labels=[\"price_clean\", \"Clean_precios\", \"conversion_en_USD\", \"conversion_en_USD_currency_USD\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7738a5fc"
   },
   "outputs": [],
   "source": [
    "precios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_GNwpW0818L"
   },
   "outputs": [],
   "source": [
    "print(\"nulos en la columna precios nueva :\", precios['precios_nuevos'].isnull().sum())\n",
    "print(\"nulos en la columna precios inicial:\", precios['price'].isnull().sum())\n",
    "properatti_df['precios_nuevos'] = precios['precios_nuevos']\n",
    "\n",
    "filtered_df_sum = properatti_df[(properatti_df['precios_nuevos'].isna())].shape[0]\n",
    "print(filtered_df_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoRaXo_NgNYP"
   },
   "outputs": [],
   "source": [
    "properatti_df[\"precios_nuevos\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7b20d38"
   },
   "outputs": [],
   "source": [
    "no_nulos_precios_usd=properatti_df[\"price_aprox_usd\"].notnull().sum()\n",
    "no_nulos_precios_usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9b2055d2"
   },
   "outputs": [],
   "source": [
    "no_nulos_precios_nuevos=properatti_df[\"precios_nuevos\"].notnull().sum()\n",
    "no_nulos_precios_nuevos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6N6jNrQ3jwQA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87a2e540"
   },
   "outputs": [],
   "source": [
    "data_filtrada2 = properatti_df[properatti_df['precios_nuevos'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fb21e698"
   },
   "outputs": [],
   "source": [
    "print(data_filtrada2[\"price_aprox_usd\"].isnull().sum())\n",
    "print(data_filtrada2[\"surface_total_clean\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b268e90e"
   },
   "outputs": [],
   "source": [
    "#Filtro los datos por precio para corroborar que este bien hecho\n",
    "data_filtrada3=properatti_df[properatti_df[\"price\"].notnull()]\n",
    "print(data_filtrada3[\"price_aprox_usd\"].isnull().sum())\n",
    "print(data_filtrada3[\"price_aprox_local_currency\"].isnull().sum())\n",
    "print(data_filtrada3[\"price_per_m2\"].isnull().sum())\n",
    "print(data_filtrada3[\"price_usd_per_m2\"].isnull().sum())\n",
    "print(data_filtrada3[\"surface_total_clean\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00d49431"
   },
   "outputs": [],
   "source": [
    "#Lleno los datos faltantes correspondientes a los precios en usd y en local currency\n",
    "properatti_df['precios_aprox_usd_clean'] = properatti_df[\"price_aprox_usd\"].fillna(properatti_df.loc[:,\"precios_nuevos\"])\n",
    "\n",
    "properatti_df['precios_aprox_local_currency_clean'] = properatti_df[\"price_aprox_usd\"].fillna(properatti_df.loc[:,\"precios_nuevos\"]*(17.6445))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9fcbbf6"
   },
   "outputs": [],
   "source": [
    "#corroboro el correcto llenado de la nueva columna precios_aprox_usd_clean\n",
    "data_filtrada4=properatti_df[properatti_df[\"precios_nuevos\"].notnull()]\n",
    "print(data_filtrada4[\"precios_aprox_usd_clean\"].isnull().sum())\n",
    "data_filtrada4['precios_aprox_local_currency_clean'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eef11bd1"
   },
   "outputs": [],
   "source": [
    "# filtro por la columna superficie limpiada\n",
    "data_filtrada5=properatti_df[properatti_df[\"surface_total_clean\"].notnull()]\n",
    "print(data_filtrada5[\"precios_nuevos\"].isnull().sum())\n",
    "print(data_filtrada5[\"price_per_m2\"].isnull().sum())\n",
    "print(data_filtrada5[\"price_usd_per_m2\"].isnull().sum())\n",
    "print(data_filtrada5[\"price_usd_per_m2\"].notnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00a9f5ef"
   },
   "outputs": [],
   "source": [
    "#filtro este dataframe datafiltrada5 por price_aprox_usd-notnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fd0edaf3"
   },
   "outputs": [],
   "source": [
    "data_filtrada_6=data_filtrada5[data_filtrada5[\"price_per_m2\"].notnull()]\n",
    "print(data_filtrada_6.shape[0])\n",
    "print(data_filtrada_6[\"precios_nuevos\"].isnull().sum())\n",
    "data_filtrada_6=data_filtrada5[data_filtrada5[\"price_usd_per_m2\"].notnull()]\n",
    "print(data_filtrada_6.shape[0])\n",
    "print(data_filtrada_6[\"precios_nuevos\"].isnull().sum())\n",
    "\n",
    "#no hay campos NaN en precios nuevos que puedan ser llenados con datos correspondientes a las otras dos columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0abdf180"
   },
   "outputs": [],
   "source": [
    "\"No puedo llenar el dataframe con datos que esten en la columna surface_total_clean y que esten en price_per_m2 o price_usd_per_m2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zjyB_UZR2hO"
   },
   "outputs": [],
   "source": [
    "# tomo la data filtrada 2 (la hecha a partir de precios nuevos y filtro por price_usd_per_m2. Se busca rellenar la columna surface_total_clean)\n",
    "\n",
    "filtro_precio_por_m2=data_filtrada2[data_filtrada2[\"price_usd_per_m2\"].notnull()]\n",
    "filtro_precio_por_m2[\"surface_total_clean\"].isnull().sum()\n",
    "\n",
    "# No puedo llenar ningùn campo correspondiente a surface_total_clean dado que en ka data filtrada, no presenta valores nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfTxvJBCS3HX"
   },
   "source": [
    "# Floor (Valentino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKb1PJ_tdhgf"
   },
   "outputs": [],
   "source": [
    "df_floor=properatti_df.floor\n",
    "#creamos variable de la columna floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIqEa1OSdq7L"
   },
   "outputs": [],
   "source": [
    "df_floor.size\n",
    "#tamanio de la columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3C0XN4i7dspk"
   },
   "outputs": [],
   "source": [
    "df_floor.isnull().sum()\n",
    "#cantidad de nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jifz5lyRdshY"
   },
   "outputs": [],
   "source": [
    "df_floor.value_counts(bins=[1,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80])\n",
    "#cantidad por rango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dofLxlcZdsaD"
   },
   "outputs": [],
   "source": [
    "df_floor.loc[df_floor>=75].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1gVHVn9dsJV"
   },
   "outputs": [],
   "source": [
    "df_floor_clean=df_floor.apply(lambda x: np.nan if x>=54 or x==np.nan else x)\n",
    "#eliminamos los valores mayores a 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0fp3IgQd0nr"
   },
   "outputs": [],
   "source": [
    "#ponemos valor 100 para cuando no tiene valor la casilla\n",
    "#esto para cuando se trate de una casa que no tiene piso o por si tenia un valor irreal\n",
    "df_floor_clean=df_floor_clean.fillna(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WltInzNdd0fl"
   },
   "outputs": [],
   "source": [
    "# A continuacion vemos que valores de propiedad se toman cuando no es nulo el valor floor\n",
    "properatti_df.loc[properatti_df.floor>=0]['property_type'].value_counts()\n",
    "# Concluimos en que los valores de apartment estan bien ya que excluimos aquellos mayores a 54\n",
    "# Pero no seria logico que el valor de una casa un PH o una store sea mayor a 1 entoces esos valores hay que pasarlos a 100 (nuestro sinonimo de nulo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwkAfZV0d0Y3"
   },
   "outputs": [],
   "source": [
    "indice=properatti_df.loc[properatti_df.floor>1][properatti_df.property_type!='apartment']['floor'].index\n",
    "indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PMLmia_d0MT"
   },
   "outputs": [],
   "source": [
    "df_floor_clean.iloc[indice]=100\n",
    "df_floor_clean.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moHFEAgetu0T"
   },
   "outputs": [],
   "source": [
    "null_count = df_floor_clean.isnull().sum()\n",
    "print(null_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdH7MT6K0uL8"
   },
   "source": [
    "# Limpieza de la columna \"expenses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e22e0d6d"
   },
   "outputs": [],
   "source": [
    "#creo el dataframe expensas para analizar\n",
    "expensas=properatti_df.loc[:,[\"expenses\", \"description\", \"title\", \"properati_url\"]]\n",
    "expensas.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1110e815"
   },
   "outputs": [],
   "source": [
    "#se calcula la cantidad de nulos en la columna expensas\n",
    "cant_nulos_expensas=expensas[\"expenses\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5d902409"
   },
   "outputs": [],
   "source": [
    "cant_nulos_expensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b15b7293"
   },
   "outputs": [],
   "source": [
    "cantidad_entries=expensas.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1f0a9742"
   },
   "outputs": [],
   "source": [
    "cantidad_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fbaf466"
   },
   "outputs": [],
   "source": [
    "#se calcula el porcentaje de nulos\n",
    "porcentaje_nulos_expensas=cant_nulos_expensas/cantidad_entries*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1897cfb"
   },
   "outputs": [],
   "source": [
    "porcentaje_nulos_expensas.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c499dab3"
   },
   "outputs": [],
   "source": [
    "expensas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf09bc49"
   },
   "outputs": [],
   "source": [
    "#busco los campos expensas que se encuentren en las columnas description, title y url\n",
    "data_filtrada = expensas[expensas[\"expenses\"].isnull()]\n",
    "regex_pattern_exp = re.compile(r'\\b(exp*)\\b', re.IGNORECASE)\n",
    "filtered_df_2 = data_filtrada[\n",
    "    data_filtrada[\"description\"].str.contains(regex_pattern_exp) |\n",
    "    data_filtrada[\"title\"].str.contains(regex_pattern_exp)\n",
    "]\n",
    "\n",
    "print(filtered_df_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "634fa341"
   },
   "outputs": [],
   "source": [
    "def extract_substring(text, regex_pattern):\n",
    "    match = re.search(regex_pattern, text)\n",
    "    if match:\n",
    "        start_index = max(0, match.start() - 5)\n",
    "        end_index = match.end() + 10\n",
    "\n",
    "        return text[start_index:end_index]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "# Use .loc to set values in the DataFrame\n",
    "print(filtered_df_2.shape)\n",
    "filtered_df_2.loc[:, 'description_short'] = filtered_df_2['description'].apply(lambda x: extract_substring(x, regex_pattern_exp))\n",
    "filtered_df_2.loc[:, 'title_short'] = filtered_df_2['title'].apply(lambda x: extract_substring(x, regex_pattern_exp))\n",
    "print(filtered_df_2.shape)\n",
    "print(filtered_df_2[\"description\"])\n",
    "print(*filtered_df_2[\"title_short\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cda05e1"
   },
   "outputs": [],
   "source": [
    "#calculo el porcentaje del campo expensas que quedaría sin llenar\n",
    "cantidad_maxima_a_agregar=(filtered_df_2.shape[0])/cantidad_entries*100\n",
    "campos_sin_llenar_en_porcentaje=porcentaje_nulos_expensas-cantidad_maxima_a_agregar\n",
    "campos_sin_llenar_en_porcentaje.round(0)\n",
    "calculate_correlation(properatti_df[properatti_df['property_type']=='apartment'],'rooms','surface_covered_in_m2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzZR7Jcr0Ixp"
   },
   "source": [
    "Los campos que contienen exp (expenses) que se extraen de la descripcion, de title y de url son, en total, a lo sumo 689.\n",
    "Representa menos del 0,6% de campos totales. Por lo tanto, quedan alrededor de 88 % de campos sin llenar.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Se descarta la columna\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qMQO0DMwULf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqdKAOKHwuZa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6a9046ad"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlsPbSoK3b-x"
   },
   "source": [
    "# Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNa-AjXuppm2"
   },
   "outputs": [],
   "source": [
    "#Definir dictionario de amenities\n",
    "sum_sinonimos={\n",
    "    'sum': 'sum',\n",
    "    's.u.m': 'sum',\n",
    "    'salón de fiestas':'sum',\n",
    "    'salón de usos múltiples':'sum'}\n",
    "estacionamiento_sinonimos={\n",
    "    'estacionamiento':'estacionamiento',\n",
    "    'cochera':'estacionamiento',\n",
    "    'garage':'estacionamiento',\n",
    "    'cocheras':'estacionamiento'}\n",
    "pileta_sinonimos= {\n",
    "    'pileta': 'pileta',\n",
    "    'piscina':'pileta'}\n",
    "parrilla_sinonimos= {\n",
    "    'parrilla':'parrilla',\n",
    "    'parrillas':'parrilla',\n",
    "    'asador':'parrilla'}\n",
    "gimnasio_sinonimos= {\n",
    "    'gym':'gimnasio',\n",
    "    'gimnasio':'gimnasio'}\n",
    "lavadero_sinonimos= {\n",
    "    'laundry':'lavadero',\n",
    "    'lavadero':'lavadero'}\n",
    "solarium_sinonimos= {\n",
    "    'solárium':'solarium',\n",
    "    'solarium':'solarium'}\n",
    "quincho_sinonimos= {\n",
    "    'quincho':'quincho'}\n",
    "baulera_sinonimos= {\n",
    "    'baulera':'baulera'}\n",
    "terraza_sinonimos= {\n",
    "    'terraza':'terraza'}\n",
    "\n",
    "amenities=['sum','estacionamiento','pileta','parrilla','gimnasio','lavadero','solarium','quincho','baulera','terraza']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYYIOt-rppjp"
   },
   "outputs": [],
   "source": [
    "#Normalizando la columna description\n",
    "properatti_df['description']=properatti_df['description'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRWlsc4P4dqU"
   },
   "outputs": [],
   "source": [
    "#Funcion para reemplazar los sinonimos de los amenities\n",
    "def amenitie_standard (df,synoms):\n",
    "    df.replace(synoms,regex=True,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33R8JCgw4wFt"
   },
   "outputs": [],
   "source": [
    "#Crear una columna que defina si tiene alguno de los amenities previamente definidos\n",
    "properatti_df['amenities']=properatti_df['description'].str.contains('|'.join(amenities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmNFNeQJ5BD4"
   },
   "outputs": [],
   "source": [
    "properatti_df[\"estacionamiento\"]=(amenitie_standard (properatti_df[\"description\"],estacionamiento_sinonimos)).str.contains('estacionamiento')\n",
    "properatti_df[\"sum\"]=(amenitie_standard (properatti_df[\"description\"],sum_sinonimos)).str.contains('sum')\n",
    "properatti_df[\"pileta\"]=(amenitie_standard (properatti_df[\"description\"],pileta_sinonimos)).str.contains('pileta')\n",
    "properatti_df[\"parrilla\"]=(amenitie_standard (properatti_df[\"description\"],parrilla_sinonimos)).str.contains('parrilla')\n",
    "properatti_df[\"gimnasio\"]=(amenitie_standard (properatti_df[\"description\"],gimnasio_sinonimos)).str.contains('gimnasio')\n",
    "properatti_df[\"lavadero\"]=(amenitie_standard (properatti_df[\"description\"],lavadero_sinonimos)).str.contains('lavadero')\n",
    "properatti_df[\"solarium\"]=(amenitie_standard (properatti_df[\"description\"],solarium_sinonimos)).str.contains('solarium')\n",
    "properatti_df[\"quincho\"]=(amenitie_standard (properatti_df[\"description\"],quincho_sinonimos)).str.contains(\"quincho\")\n",
    "properatti_df[\"baulera\"]=(amenitie_standard (properatti_df[\"description\"],baulera_sinonimos)).str.contains(\"baulera\")\n",
    "properatti_df[\"terraza\"]=(amenitie_standard (properatti_df[\"description\"],terraza_sinonimos)).str.contains(\"terraza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVS4EOPf5bLw"
   },
   "outputs": [],
   "source": [
    "#Validar la creacion de las nuevas columnas\n",
    "properatti_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAe1LBlS9sQD"
   },
   "outputs": [],
   "source": [
    "#Pocentaje de propiedades con amenities\n",
    "amenities= properatti_df[properatti_df['amenities']==True]\n",
    "amenities['amenities'].count()/properatti_df['amenities'].count()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6As6yUQCFI-"
   },
   "outputs": [],
   "source": [
    "#Porcentaje de tipos de propiedades con amenities\n",
    "amenities['property_type'].value_counts()/properatti_df['property_type'].value_counts()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiYa_Ckoqcr4"
   },
   "source": [
    "# Correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrbQBJjgpp19"
   },
   "outputs": [],
   "source": [
    "#filtro el dataframe por provincia\n",
    "grouped_df = properatti_df.groupby('provincia').agg({'precios_nuevos': 'mean', 'surface_total_clean': 'mean'}).reset_index()\n",
    "\n",
    "#creo el gráfico de dispersión utilizando los datos agrupados\n",
    "plt.scatter(grouped_df['surface_total_clean'], grouped_df['precios_nuevos'])\n",
    "plt.title('Diagrama de Dispersión Promedio entre Precio y Superficie Total por Ubicación')\n",
    "plt.xlabel('Superficie Total Promedio por Ubicación')\n",
    "plt.ylabel('Precio Promedio por Ubicación')\n",
    "plt.xlim(0, 700)\n",
    "plt.ylim(0, 600000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XJQQ9w5IIgj"
   },
   "outputs": [],
   "source": [
    "#correlaciones de precio con las variables superficie, rooms y precio por m2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyt4YdkYpps4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa1a4a79"
   },
   "outputs": [],
   "source": [
    "#Creo una mascara para cada una de las provincias\n",
    "masks = [\n",
    "    properatti_df[\"provincia\"]=='Buenos Aires',\n",
    "    properatti_df[\"provincia\"]=='Cordoba',\n",
    "    properatti_df[\"provincia\"]=='Santa Fe',\n",
    "    properatti_df[\"provincia\"]=='Rio Negro',\n",
    "    properatti_df[\"provincia\"]=='Neuquen',\n",
    "    properatti_df[\"provincia\"]=='Tucuman',\n",
    "    properatti_df[\"provincia\"]=='Mendoza',\n",
    "    properatti_df[\"provincia\"]=='Corrientes',\n",
    "    properatti_df[\"provincia\"]=='Misiones',\n",
    "    properatti_df[\"provincia\"]=='Entre Rios',\n",
    "    properatti_df[\"provincia\"]=='Salta',\n",
    "    properatti_df[\"provincia\"]=='Chubut',\n",
    "    properatti_df[\"provincia\"]=='San Luis',\n",
    "    properatti_df[\"provincia\"]=='La Pampa',\n",
    "    properatti_df[\"provincia\"]=='Formosa',\n",
    "    properatti_df[\"provincia\"]=='Chaco',\n",
    "    properatti_df[\"provincia\"]=='San Juan',\n",
    "    properatti_df[\"provincia\"]=='Tierra Del Fuego',\n",
    "    properatti_df[\"provincia\"]=='Catamarca',\n",
    "    properatti_df[\"provincia\"]=='Jujuy',\n",
    "    properatti_df[\"provincia\"]=='Santa Cruz',\n",
    "    properatti_df[\"provincia\"]=='Santiago Del Estero',\n",
    "    properatti_df[\"provincia\"]=='La Rioja']\n",
    "# Variables para calcular la correlación\n",
    "variable1 = 'precios_nuevos'\n",
    "variable2 = 'surface_total_clean'\n",
    "variable3 = \"price_usd_per_m2\"\n",
    "variable4 = \"rooms_clean\"\n",
    "\n",
    "# Se Crean las listas para guardar los resultados y un bucle for para aplicar las máscaras y calcular la correlación\n",
    "lista_correlacion1=[]\n",
    "lista_correlacion2=[]\n",
    "lista_correlacion3=[]\n",
    "for i, mask in enumerate(masks):\n",
    "    # Se Aplica la máscara al DataFrame\n",
    "    filtered_df = properatti_df[mask]\n",
    "\n",
    "    # Se calcula la correlación entre dos variables especificadas\n",
    "    correlation1 = calculate_correlation (filtered_df, 'precios_nuevos', 'surface_total_clean')\n",
    "    correlation2 = calculate_correlation (filtered_df, 'precios_nuevos', 'price_usd_per_m2')\n",
    "    correlation3 = calculate_correlation (filtered_df, 'precios_nuevos', 'rooms_clean')\n",
    "\n",
    "    lista_correlacion1.append(correlation1)\n",
    "    lista_correlacion2.append(correlation2)\n",
    "    lista_correlacion3.append(correlation3)\n",
    "\n",
    "\n",
    "# Se imprime la correlación para cada máscara\n",
    "\n",
    "\n",
    "print(lista_correlacion1)\n",
    "print(lista_correlacion2)\n",
    "print(lista_correlacion3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e12c7db6"
   },
   "outputs": [],
   "source": [
    "#se crea una lista con los nombres de las provincias\n",
    "lista_provincias= [\n",
    "    'Buenos Aires',\n",
    "    'Cordoba',\n",
    "    'Santa Fe',\n",
    "    'Rio Negro',\n",
    "    'Neuquen',\n",
    "    'Tucuman',\n",
    "    'Mendoza',\n",
    "    'Corrientes',\n",
    "    'Misiones',\n",
    "    'Entre Rios',\n",
    "    'Salta',\n",
    "    'Chubut',\n",
    "    'San Luis',\n",
    "    'La Pampa',\n",
    "    'Formosa',\n",
    "    'Chaco',\n",
    "    'San Juan',\n",
    "    'Tierra Del Fuego',\n",
    "    'Catamarca',\n",
    "    'Jujuy',\n",
    "    'Santa Cruz',\n",
    "    'Santiago Del Estero',\n",
    "    'La Rioja']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae2a61f0"
   },
   "outputs": [],
   "source": [
    "#se crean tres diccionarios, se los transforma en 3 dataframes. Luego se copian las columnas de correlacion al primer dataframe\n",
    "diccionario_correlacion_precio_superficie_en_m2=dict(zip(lista_provincias,lista_correlacion1))\n",
    "diccionario_correlacion_precio_precio_usd_por_m2=dict(zip(lista_provincias,lista_correlacion2))\n",
    "diccionario_correlacion_precio_rooms=dict(zip(lista_provincias,lista_correlacion3))\n",
    "\n",
    "correlacion1_df=pd.DataFrame(list(diccionario_correlacion_precio_superficie_en_m2.items()), columns=[\"Provincia\", \"Correlacion_superficie\"])\n",
    "\n",
    "\n",
    "correlacion2_df=pd.DataFrame(list(diccionario_correlacion_precio_precio_usd_por_m2.items()), columns=[\"Provincia\", \"Correlacion_precio_usd_por_m2\"])\n",
    "\n",
    "\n",
    "correlacion3_df=pd.DataFrame(list(diccionario_correlacion_precio_rooms.items()), columns=[\"Provincia\", \"Correlacion_rooms\"])\n",
    "\n",
    "\n",
    "\n",
    "correlacion1_df[\"Correlacion_precio_usd_por_m2\"]=correlacion2_df[\"Correlacion_precio_usd_por_m2\"]\n",
    "correlacion1_df[\"Correlacion_rooms\"]= correlacion3_df[\"Correlacion_rooms\"]\n",
    "correlacion1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akuuk_h6UMq0"
   },
   "outputs": [],
   "source": [
    "#grafico para comparar los nulos antes y después de la limpieza\n",
    "\n",
    "lista4 = ['precios_nuevos', 'price', 'surface_total_clean', 'surface_total_in_m2', 'rooms_clean', 'rooms', 'precios_aprox_usd_clean', 'price_aprox_usd']\n",
    "\n",
    "valores4 = [properatti_df['precios_nuevos'].isnull().sum()/(properatti_df.shape[0])*100,\n",
    "            properatti_df['price'].isnull().sum()/(properatti_df.shape[0])*100,\n",
    "            properatti_df['surface_total_clean'].isnull().sum()/(properatti_df.shape[0])*100,\n",
    "            properatti_df['surface_total_in_m2'].isnull().sum()/(properatti_df.shape[0])*100,\n",
    "            properatti_df['rooms_clean'].isnull().sum()/(properatti_df.shape[0])*100,\n",
    "            properatti_df['rooms'].isnull().sum()/(properatti_df.shape[0])*100,\n",
    "            properatti_df['precios_aprox_usd_clean'].isnull().sum()/(properatti_df.shape[0])*100,\n",
    "            properatti_df['price_aprox_usd'].isnull().sum()/(properatti_df.shape[0])*100, ]\n",
    "\n",
    "colores = ['blue', 'green', 'blue', 'green', 'blue', 'green', 'blue', 'green']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(len(lista4)):\n",
    "    ax.bar(i, valores4[i], color=colores[i], label=lista4[i])\n",
    "\n",
    "ax.set(xlabel=\"Columna\", ylabel=\"Porcentaje de nulos\", title=\"Compararión de porcentaje de nulos\")\n",
    "plt.xticks(np.arange(len(lista4)), lista4, rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWDCj8cLUMmT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
